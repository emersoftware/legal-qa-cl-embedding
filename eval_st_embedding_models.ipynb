{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos Sentence Transformers (sentence similarity) para generar Text Embreddings para busqueda y recuperación asimetrica\n",
    "\n",
    "En este cuaderno se evaluan diferentes modelos de procesamiento de lenguaje natural de similitud de sentencias para validar el entrenamiento de Tulio y Beto en el caso de uso de recuperar normas de la ley chilena comparando su titulo con preguntas.\n",
    "\n",
    "Para ellos:\n",
    "* con cada modelo se genera una base de datos vectorial con los text embeddings\n",
    "* dado un set de datos de preguntas y respuestas que contiene las ids de las normas relevantes se hacen busquedas en la base de datos\n",
    "* para cada pregunta se recupera de forma arbitraria las k normas (titulos de las normas representados como vectores) que tienen menor distancia coseno con la representación vectorial de la pregunta. k es el numero de normas relevantes en el set de datos para la pregunta.\n",
    "* se calcula la precision de cada modelo como el promedio de la precision de cada recuperacion, esta es el porcentaje de ids de las normas recuperadas que se encuentran en el conjunto de ids relevantes del set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para calcular precision de una recuperacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(retrieved_ids, relevant_ids):\n",
    "    true_positives = retrieved_ids.intersection(relevant_ids)\n",
    "    if len(retrieved_ids) == 0:\n",
    "        return 0\n",
    "    return len(true_positives) / len(retrieved_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraciónd el set de datos de evaluación, modelos a evaluar y metricas finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./qa_dataset_legal_filter_heuristic_curated.csv')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "devide = 'mps' if torch.backends.mps.is_available() else device\n",
    "\n",
    "model_names = [ \n",
    "                'sentence-transformers/all-MiniLM-L6-v2', #Modelo por defecto de ChromaDB\n",
    "                #Sentence Transformers en español desde HuggingFace\n",
    "                'Maite89/Roberta_finetuning_semantic_similarity_stsb_multi_mt',\n",
    "                'eduardofv/stsb-m-mt-es-distilbert-base-uncased',\n",
    "                'eduardofv/stsb-m-mt-es-distiluse-base-multilingual-cased-v1',\n",
    "                'hiiamsid/sentence_similarity_spanish_es',\n",
    "                'mrm8488/distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es',\n",
    "                'hackathon-pln-es/bertin-roberta-base-finetuning-esnli',\n",
    "                'hackathon-pln-es/paraphrase-spanish-distilroberta',\n",
    "                'jaimevera1107/all-MiniLM-L6-v2-similarity-es',\n",
    "                'cnrhs/sen-sim-es',\n",
    "                'dariolopez/roberta-base-bne-finetuned-msmarco-qa-es',\n",
    "                'dariolopez/roberta-base-bne-finetuned-msmarco-qa-es-mnrl-mn',\n",
    "                #Modelos de este trabajo\n",
    "                #'emersoftware/test',\n",
    "                'emersoftware/tulio-st-msmarco-es-mnrl',\n",
    "                'emersoftware/beto-st-msmarco-es-mnrl',\n",
    "                'emersoftware/tulio-st-msmarco-es-mnrl-v2',\n",
    "                'emersoftware/beto-st-msmarco-es-mnrl-v2',\n",
    "                #'emersoftware/tulio-chilean-spanish-bert-msmarco-qa-es-mnrl-mn',\n",
    "                #'emersoftware/tulio-chilean-spanish-bert-finetuned-msmarco-qa-es',\n",
    "                #Modelos multilingües Sentence Transformers\n",
    "                'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                'sentence-transformers/LaBSE',\n",
    "                'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
    "                'sentence-transformers/distiluse-base-multilingual-cased-v1'\n",
    "]\n",
    "\n",
    "metrics_results = {model_name: {'precision': []} for model_name in model_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de los documentos para generar las bases de datos vectoriales\n",
    "Cada uno de los documentos es el titulo de una norma que tiene como metadatos las ids de las normas relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 784/784 [00:03<00:00, 204.88it/s]\n"
     ]
    }
   ],
   "source": [
    "def metadata_func (record: dict, metadata: dict) -> dict:\n",
    "    metadata['idNorma'] = record.get('idNorma')\n",
    "    return metadata\n",
    "\n",
    "loader = DirectoryLoader('data_normas_json/',\n",
    "                         show_progress=True,\n",
    "                         use_multithreading=True,\n",
    "                         max_concurrency=8,\n",
    "                         loader_cls=JSONLoader,\n",
    "                         loader_kwargs={\n",
    "                             'jq_schema': '.',\n",
    "                             'content_key': 'titulo',\n",
    "                             'metadata_func': metadata_func\n",
    "                            })\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de cada uno de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo: Maite89/Roberta_finetuning_semantic_similarity_stsb_multi_mt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo: eduardofv/stsb-m-mt-es-distilbert-base-uncased\n",
      "Evaluando modelo: eduardofv/stsb-m-mt-es-distiluse-base-multilingual-cased-v1\n",
      "Evaluando modelo: hiiamsid/sentence_similarity_spanish_es\n",
      "Evaluando modelo: mrm8488/distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es\n",
      "Evaluando modelo: hackathon-pln-es/bertin-roberta-base-finetuning-esnli\n",
      "Evaluando modelo: hackathon-pln-es/paraphrase-spanish-distilroberta\n",
      "Evaluando modelo: jaimevera1107/all-MiniLM-L6-v2-similarity-es\n",
      "Evaluando modelo: cnrhs/sen-sim-es\n",
      "Evaluando modelo: dariolopez/roberta-base-bne-finetuned-msmarco-qa-es\n",
      "Evaluando modelo: dariolopez/roberta-base-bne-finetuned-msmarco-qa-es-mnrl-mn\n",
      "Evaluando modelo: emersoftware/tulio-st-msmarco-es-mnrl\n",
      "Evaluando modelo: emersoftware/beto-st-msmarco-es-mnrl\n",
      "Evaluando modelo: emersoftware/tulio-st-msmarco-es-mnrl-v2\n",
      "Evaluando modelo: emersoftware/beto-st-msmarco-es-mnrl-v2\n",
      "Evaluando modelo: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "Evaluando modelo: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Evaluando modelo: sentence-transformers/LaBSE\n",
      "Evaluando modelo: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "Evaluando modelo: sentence-transformers/distiluse-base-multilingual-cased-v1\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    print(f\"Evaluando modelo: {model_name}\")\n",
    "    \n",
    "    # Cargar y generar embeddings con el modelo actual\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': device})\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=f'vectordb/{model_name}', collection_name='titulos', collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Comprobar si 'idNormas' es una cadena válida\n",
    "        if pd.notna(row['idNormas']):\n",
    "            query = row['pregunta']\n",
    "            query = query.lower()\n",
    "            query = query.replace('?', '').replace('¿', '').replace('!', '')\n",
    "            \n",
    "\n",
    "            relevant_ids = set(row['idNormas'].split(';'))\n",
    "\n",
    "            retrieved_docs = db.similarity_search(query, k=len(relevant_ids))\n",
    "            \n",
    "            retrieved_ids = set(doc.metadata['idNorma'] for doc in retrieved_docs)\n",
    "\n",
    "            precision = calculate_precision(retrieved_ids, relevant_ids)\n",
    "            \n",
    "            # Guardar métricas\n",
    "            metrics_results[model_name]['precision'].append(precision)\n",
    "\n",
    "        else:\n",
    "            # Saltar esta fila si 'idNormas' no es una cadena válida\n",
    "            print(f\"Fila {index} omitida: idNormas es NaN o no válido.\")\n",
    "            continue  # Continuar con la siguiente iteración del bucles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se muestran los resultados de la evaluación\n",
    "Precision promedio de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Maite89/Roberta_finetuning_semantic_similarity_stsb_multi_mt \t\t Precision: 0.2651335038127491 \t \n",
      "Modelo: eduardofv/stsb-m-mt-es-distilbert-base-uncased \t\t Precision: 0.18499406777708666 \t \n",
      "Modelo: eduardofv/stsb-m-mt-es-distiluse-base-multilingual-cased-v1 \t\t Precision: 0.3063218490105282 \t \n",
      "Modelo: hiiamsid/sentence_similarity_spanish_es \t\t Precision: 0.35417300205036056 \t \n",
      "Modelo: mrm8488/distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es \t\t Precision: 0.23016564148639623 \t \n",
      "Modelo: hackathon-pln-es/bertin-roberta-base-finetuning-esnli \t\t Precision: 0.2926814799456309 \t \n",
      "Modelo: hackathon-pln-es/paraphrase-spanish-distilroberta \t\t Precision: 0.34594218674407357 \t \n",
      "Modelo: jaimevera1107/all-MiniLM-L6-v2-similarity-es \t\t Precision: 0.24186450342110716 \t \n",
      "Modelo: cnrhs/sen-sim-es \t\t Precision: 0.35375371483862045 \t \n",
      "Modelo: dariolopez/roberta-base-bne-finetuned-msmarco-qa-es \t\t Precision: 0.2284735180961596 \t \n",
      "Modelo: dariolopez/roberta-base-bne-finetuned-msmarco-qa-es-mnrl-mn \t\t Precision: 0.3394521620936715 \t \n",
      "Modelo: emersoftware/tulio-st-msmarco-es-mnrl \t\t Precision: 0.37284539360011054 \t \n",
      "Modelo: emersoftware/beto-st-msmarco-es-mnrl \t\t Precision: 0.37347432441772066 \t \n",
      "Modelo: emersoftware/tulio-st-msmarco-es-mnrl-v2 \t\t Precision: 0.3593541940240053 \t \n",
      "Modelo: emersoftware/beto-st-msmarco-es-mnrl-v2 \t\t Precision: 0.36084560093994056 \t \n",
      "Modelo: sentence-transformers/paraphrase-multilingual-mpnet-base-v2 \t\t Precision: 0.33423439767779395 \t \n",
      "Modelo: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 \t\t Precision: 0.30854931232289723 \t \n",
      "Modelo: sentence-transformers/LaBSE \t\t Precision: 0.27463571543760223 \t \n",
      "Modelo: sentence-transformers/distiluse-base-multilingual-cased-v2 \t\t Precision: 0.2579428433202018 \t \n",
      "Modelo: sentence-transformers/distiluse-base-multilingual-cased-v1 \t\t Precision: 0.2868811482019029 \t \n"
     ]
    }
   ],
   "source": [
    "for model_name, metrics in metrics_results.items():\n",
    "    avg_precision = np.mean(metrics['precision'])\n",
    "    print(f\"Modelo: {model_name} \\t\\t Precision: {avg_precision} \\t \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo con la mejor precisión promedio: emersoftware/beto-st-msmarco-es-mnrl\n",
      "Valores de precisión para este modelo:\n",
      "0.25\n",
      "0.2\n",
      "0.5\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.25\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.0\n",
      "0.0\n",
      "0.6\n",
      "0.0\n",
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.6\n",
      "0.2\n",
      "0.5\n",
      "0.8\n",
      "0.0\n",
      "0.5\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.5\n",
      "0.75\n",
      "0.25\n",
      "0.5\n",
      "0.0\n",
      "1.0\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.2\n",
      "0.6\n",
      "0.6\n",
      "1.0\n",
      "0.5\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.6666666666666666\n",
      "0.2\n",
      "0.25\n",
      "0.2\n",
      "0.25\n",
      "0.3333333333333333\n",
      "0.4\n",
      "0.0\n",
      "0.4\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.25\n",
      "0.8\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "1.0\n",
      "0.5\n",
      "0.0\n",
      "0.0\n",
      "0.6\n",
      "0.2\n",
      "0.75\n",
      "0.5\n",
      "0.0\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.25\n",
      "0.0\n",
      "0.4\n",
      "0.0\n",
      "0.5\n",
      "0.0\n",
      "0.0\n",
      "0.25\n",
      "0.75\n",
      "0.8571428571428571\n",
      "0.3333333333333333\n",
      "0.75\n",
      "0.5\n",
      "0.5\n",
      "0.2\n",
      "0.2\n",
      "0.0\n",
      "0.0\n",
      "0.2\n",
      "0.3333333333333333\n",
      "0.25\n",
      "0.0\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.25\n",
      "0.2857142857142857\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.6666666666666666\n",
      "1.0\n",
      "1.0\n",
      "0.5\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.3333333333333333\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.6\n",
      "0.25\n",
      "0.0\n",
      "1.0\n",
      "0.6\n",
      "0.3333333333333333\n",
      "0.25\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.25\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.16666666666666666\n",
      "0.3333333333333333\n",
      "0.4\n",
      "0.0\n",
      "0.25\n",
      "0.0\n",
      "0.6666666666666666\n",
      "0.4\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.8\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.2\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.0\n",
      "0.6666666666666666\n",
      "0.2\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.0\n",
      "0.2\n",
      "0.0\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.3333333333333333\n",
      "1.0\n",
      "0.0\n",
      "0.25\n",
      "0.5\n",
      "0.0\n",
      "0.3076923076923077\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.0\n",
      "0.5\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.25\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.25\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.2\n",
      "0.0\n",
      "0.0\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.75\n",
      "0.25\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "0.25\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.7142857142857143\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.8\n",
      "0.5\n",
      "0.0\n",
      "0.0\n",
      "0.4\n",
      "0.5\n",
      "0.25\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.0\n",
      "0.3333333333333333\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.0\n",
      "0.75\n",
      "0.0\n",
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.0\n",
      "0.2\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.5\n",
      "0.5\n",
      "0.4\n",
      "1.0\n",
      "0.0\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.5\n",
      "0.25\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.25\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calcula la precisión promedio para cada modelo\n",
    "average_precision = {model: np.mean(metrics['precision']) for model, metrics in metrics_results.items()}\n",
    "\n",
    "# Encuentra el modelo con la mejor precisión promedio\n",
    "best_model = max(average_precision, key=average_precision.get)\n",
    "\n",
    "# Imprime el nombre del modelo con mejor precisión y todos sus valores de precisión\n",
    "print(f\"Modelo con la mejor precisión promedio: {best_model}\")\n",
    "print(\"Valores de precisión para este modelo:\")\n",
    "for precision in metrics_results[best_model]['precision']:\n",
    "    print(precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
